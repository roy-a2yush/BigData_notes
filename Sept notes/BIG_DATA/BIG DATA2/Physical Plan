
val dfnext = spark.sql("select level, date_format(datetime, 'MMMM') as month, first(cast(date_format(datetime, 'M') as int)) as monthNum, count(1) as total from logging_view group by level, month order by monthNum,level")
dfnext.explain
dfnext.show()

== Physical Plan ==
*(3) Sort [monthNum#16 ASC NULLS FIRST, level#10 ASC NULLS FIRST], true, 0
+- Exchange rangepartitioning(monthNum#16 ASC NULLS FIRST, level#10 ASC NULLS FIRST, 200)
   +- *(2) HashAggregate(keys=[level#10, date_format(cast(datetime#11 as timestamp), MMMM, Some(Asia/Kolkata))#29], functions=[first(cast(date_format(cast(datetime#11 as timestamp), M, Some(Asia/Kolkata)) as int), false), count(1)])
      +- Exchange hashpartitioning(level#10, date_format(cast(datetime#11 as timestamp), MMMM, Some(Asia/Kolkata))#29, 200)
         +- *(1) HashAggregate(keys=[level#10, date_format(cast(datetime#11 as timestamp), MMMM, Some(Asia/Kolkata)) AS date_format(cast(datetime#11 as timestamp), MMMM, Some(Asia/Kolkata))#29], functions=[partial_first(cast(date_format(cast(datetime#11 as timestamp), M, Some(Asia/Kolkata)) as int), false), partial_count(1)])
            +- *(1) FileScan csv [level#10,datetime#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/roya2yush/Desktop/biglog1.txt], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<level:string,datetime:string>


point to be noted: HashAggregae
------------------------------------------------------------------------------------------------------------------------------



val dfnext2=spark.sql("select level, date_format(datetime,'MMMM') as month, count(1) as total from logging_view group by level, month order by cast(first(date_format(datetime,'M'))as int), level")
dfnext2.explain
dfnext2.show()

== Physical Plan ==
*(3) Project [level#10, month#14, total#15L]
+- *(3) Sort [aggOrder#25 ASC NULLS FIRST, level#10 ASC NULLS FIRST], true, 0
   +- Exchange rangepartitioning(aggOrder#25 ASC NULLS FIRST, level#10 ASC NULLS FIRST, 200)
      +- SortAggregate(key=[level#10, date_format(cast(datetime#11 as timestamp), MMMM, Some(Asia/Kolkata))#27], functions=[count(1), first(date_format(cast(datetime#11 as timestamp), M, Some(Asia/Kolkata)), false)])
         +- *(2) Sort [level#10 ASC NULLS FIRST, date_format(cast(datetime#11 as timestamp), MMMM, Some(Asia/Kolkata))#27 ASC NULLS FIRST], false, 0
            +- Exchange hashpartitioning(level#10, date_format(cast(datetime#11 as timestamp), MMMM, Some(Asia/Kolkata))#27, 200)
               +- SortAggregate(key=[level#10, date_format(cast(datetime#11 as timestamp), MMMM, Some(Asia/Kolkata)) AS date_format(cast(datetime#11 as timestamp), MMMM, Some(Asia/Kolkata))#27], functions=[partial_count(1), partial_first(date_format(cast(datetime#11 as timestamp), M, Some(Asia/Kolkata)), false)])
                  +- *(1) Sort [level#10 ASC NULLS FIRST, date_format(cast(datetime#11 as timestamp), MMMM, Some(Asia/Kolkata)) AS date_format(cast(datetime#11 as timestamp), MMMM, Some(Asia/Kolkata))#27 ASC NULLS FIRST], false, 0
                     +- *(1) FileScan csv [level#10,datetime#11] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/roya2yush/Desktop/biglog1.txt], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<level:string,datetime:string>


point to be noted: SortAggregate

CONCLUSION: sort aggregate is slower, hashaggregate is faster.

EXPLANATION:
	level	month 		count 	monthNum
	warn	jan			1		1
	warn	jan			1		1
	warn	jan			1		1
	error	march		1		3
	warn	jan			1		1

		sort aggregate brings all the levels together, which brings all the warn and its row together.
			the time compllexity for best sorting is nlog(n), which means, if the data is doubled, the time will be more than doubled. Hence, we are spending huge amount of time as the data increases.

		hash aggregate uses hash function.
			It costs a little bit of extra memory for storing the hash table.
			So, at the cost of the a little memory we are reducing sorting. Time complexity is O(n)
			Heavy optimisation technique.
			The value which we have has to be mutable. in the above example count is mutable.

Make sure that values are of mutable type so that spark internally uses, hash aggregate.


Mutable types:
	NullType,
	BooleanType,
	ByteType,
	ShortType,
	IntegerType,
	LongType,
	FloatType,
	DoubleType,
	DateType,
	TimestampType


Concept of unsafe memory or unsafe row:
	hash aggregation makes use of unsafe memory to grab a section of memory outside jvm.